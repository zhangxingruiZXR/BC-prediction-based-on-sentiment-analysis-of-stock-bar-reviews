# 代码细节和相关技术

## 采集数据

采集数据环节，原本使用的是我自己写的爬虫，发现不够稳定，后续应用阶段就用了别人写的爬虫。

我们使用了BeautifulSoup来构建爬虫，下面叙述具体过程。

```python
#分为不同页面爬取
for i in range(250,1,-1):
        print(i)
        share_code=[]
        share_code.append({'share_code':'002230','page':i})#传输必要的信息爬取
        run(share_code)#爬取
        print('-------sleeping-------')
        time.sleep(random.randint(20,40))#停止一下防止封锁ip
```

```python
def run(data):
        html = getHtml("https://guba.eastmoney.com/list,"+data[0]['share_code']+"_" + str(data[0]['page']) + ".html")#网页网址构成
        getAndStoreInf(html,data['share_code'])
        
def getAndStoreInf(html,share_code):
    soup = BeautifulSoup(html, "html.parser")
    contain = soup.find_all("tr", {"class": "listitem"})#获取每条评论节点
    dataCSV = open('c:/Users/zhang/Desktop/面向科学问题的编程实践/news.csv', 'a', newline='', encoding='utf-8')
    headerCSV = ['comment_id', 'title', 'readcount', 'date', 'user_id', 'reply_count']
    writer = csv.DictWriter(dataCSV,headerCSV)
    for i in contain:
        try:
            if (i != None) and (i.find("em", {"class": "hinfo"}) == None and i.find("em", {"class": "settop"}) == None and "id" not in i.attrs.keys()):#排除掉官方新闻等
                content = i.find("div", {"class": "title"}).find("a")
                contentUrl="http://guba.eastmoney.com"+content["href"]#内容详情页面
                readcount = i.find("div", {"class": "read"}).get_text()
                replycount = i.find("div", {"class": "reply"}).get_text()
                datecount = i.find("div", {"class": "update"}).get_text()
                datecount = datecount[0:5]
                userUrl = i.find("div", {"class": "author"}).find("a").attrs["href"]
                userId=userUrl[23:]
                userUrl = "https:" + userUrl
                text=content.get_text()#获取评论标题。
                if "caifuhao" in str(contentUrl):#排除相关资讯
                     continue
                commentId=content["href"][-14:-5]
                if contentUrl.__contains__("qa") or contentUrl.__contains__("cfhpl"):#排除问答等内容，只保留用户评论
                    continue
                if userUrl=="http://guba.eastmoney.com/list,jjdt.html":
                    continue
                comment={"readcount":readcount,"reply_count":replycount,"date":datecount,"title":text}
                writer.writerow(comment)        
        except:
            continue
```

从而我们获得了每条评论的阅读量，标题，评论数量。

## 对每条评论分析情绪值

使用snownlp分析。

先要用数据训练一个新的snownlp出来。

```python
from snownlp import sentiment
sentiment.train('./neg.txt', './pos.txt')
sentiment.save('sentiment.marshal')#保存好新训练的模型
```



```python
for row in dataset2.itertuples():#dataset就是上文中获得的数据
    temp1=str(row[5])
    temp4=temp1[0:5]
    temp22=str(row[4])
    temp2 = SnowNLP(temp22)#构建snownlp对象
    temp3 = temp2.sentiments#分析情绪值
    writer.writerow([row[2], row[3], temp3, temp4])#输出到新的文本里
```

## 构建情绪指标

```python
#构建平均值指标（文中第一种指标）
datepast = date.drop_duplicates(keep='last')#日期去重
for i in datepast:
    tot1 = 0
    tot2 = 0
    for row in dataset.iterrows():
        if(row[1]["时间"] == i):
            tot1 = tot1 + 1
            tot2 = tot2 + row[1]["指数"]
    result = tot2 / tot1
    writer.writerow([result, i])
    
#构建目前常用的指标（文中第二类指标）
for i in datelist:
    pos = 0
    neg = 0
    for row in dataset.iterrows():
        if(row[1]["时间"] == i):
            if(row[1]["指数"] > 0.5):
                pos = pos + 1
            else:
                neg = neg + 1
    temp = (1+pos)/(1+neg)
    result = math.log(temp, math.e)
    writer.writerow([i, neg, pos, result])
    
#构建改良的指标（文中第三类指标）
for i in datelist:
    pos = 0
    neg = 0
    for row in dataset.iterrows():
        if(row[1]["时间"] == i):
            if(row[1]["指数"] > 0.5):
                pos = pos + (row[1]["评论"]) * 10 + float(row[1]["阅读"])
            else:
                neg = neg + (row[1]["评论"]) * 10 + float(row[1]["阅读"])
    temp = (1+pos)/(1+neg)
    result = math.log(temp, math.e)
    writer.writerow([i, neg, pos, result])
```

## 分析指标与当日涨跌幅度的相关性

一个好的指标应该与当日的涨跌幅显著相关。

```python
# 使用的是东财数据接口获取历史股票数据
import akshare as ak
stock = ak.stock_zh_a_hist(symbol="002230", period="daily", start_date="20231210", end_date='20240316', adjust="")
print(stock)
#dataset内存储了股票数据与情绪指标
for col in list(dataset):
    if( col == "neg" or col == "pos" or col == "情绪" or col == "date" or col == "日期"):
        continue
    my_rho = np.corrcoef(dataset["情绪"], dataset[col])#使用corrcoef分析相关性
    rho.append(my_rho[0][1])
    x.append(col)
    if((my_rho[0][1] * math.sqrt(63 / (1 - my_rho[0][1]**2))) >= 1.96):#判断相关性显著程度
        pvalue.append("*")
    else: pvalue.append("\\")
print(rho)
```

## 数据填充处理

```python
#先插入需要填充的日期
for row in dataset.index:
    for rowi in stock.iterrows():
        if (str(rowi[1]["日期"])[5:10] == dataset.loc[row,"date"]):
            for i in list(stock):
                if(i == "日期"):
                    continue
                dataset.loc[row, i] = rowi[1][i]
#使用Knnimputer插入值
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors = 2)#此处采用了2，发现在这个数的时候每日情绪指标与其相关性较高
X = dataset.iloc[:,1:]
res = imputer.fit_transform(X)
```

## 模型预测

均采用监督学习。用于训练的数据有前两天的情绪指标和前一天的涨跌幅。

```python
#以下展示LinearSVC的训练过程，其他训练同理
dataset = pd.read_csv("拟合数据.csv")
#读入数据划分数据和label，注意在这个训练中我们将任务分成了预测明天的涨或者跌，是一个二分类任务。
X = dataset.iloc[:,1:4]
y = dataset.iloc[:,-1]
#划分训练集和测试集，随机种子为42好复现实验
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)
#标准化，本次所有模型的数据均是标准化的，这有助于统一数据格式
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
#使用5-fold cross validation调试参数
model_SVM = svm.LinearSVC(C=1.0,multi_class = "crammer_singer",loss = "hinge",penalty = "l1",dual = 'auto', fit_intercept = True)
model_SVM.fit(X_train,y_train)
from pprint import pprint
param_grid = {
             "fit_intercept": [True,False],
             "dual": ["auto",True,False],
             "C": [1,10,0.1,0.01]}
grid_search = GridSearchCV(model_SVM,param_grid,cv = 5, scoring = 'accuracy',n_jobs= 12)
grid_search.fit(X_train,y_train)
best_params = grid_search.best_params_
print(best_params)
#测试集准确率
clf = svm.LinearSVC(dual="auto",C = 1,fit_intercept = False)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print('Accuracy: ', accuracy_score(y_test, y_pred))
```

下面给出每个模型的具体参数。

```python
#以下为未填充数据的
#Linear SVC
clf = svm.LinearSVC(dual="auto",C = 1,fit_intercept = False)
#SVC
clfSVC = svm.SVC(C = 0.1,kernel= 'rbf')
#LogReg
Logis = LogisticRegression()
#Naive Bayes
model_bnl = BernoulliNB(binarize=0.7).fit(X_train, y_train)
#KNN
model_KNN_latest = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)

#以下为填充数据的
#Linear SVC
clf = svm.LinearSVC(dual="auto",C = 1,fit_intercept = True)
#SVC
clfSVC = svm.SVC(kernel = 'rbf',C = 1)
#LogReg
Logis = LogisticRegression()
#Naive Bayes
model_bnl = BernoulliNB(binarize=0.7).fit(X_train, y_train)
#KNN
model_KNN_latest = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)
```

下面是线性模型的应用，不是二分类任务，预测涨跌真实值。

```python
#相关函数
def stdError_func(y_test, y):
  return np.sqrt(np.mean((y_test - y) ** 2))

#下面是两种评价方式
def R2_1_func(y_test, y):
  return 1 - ((y_test - y) ** 2).sum() / ((y.mean() - y) ** 2).sum()


def R2_2_func(y_test, y):
  y_mean = np.array(y)
  y_mean[:] = y.mean()
  return 1 - stdError_func(y_test, y) / stdError_func(y_mean, y)
#训练与预测
x = np.array(X.values)

y = np.array(y.values)

cft = linear_model.LinearRegression()
print(x.shape)
cft.fit(x, y)

print("model coefficients", cft.coef_)
print("model intercept", cft.intercept_)


predict_y =  cft.predict(x)
strError = stdError_func(predict_y, y)
R2_1 = R2_1_func(predict_y, y)
R2_2 = R2_2_func(predict_y, y)
score = cft.score(x, y) ##sklearn中自带的模型评估，与R2_1逻辑相同
#strError=2.93, R2_1=0.11,  R2_2=0.06, clf.score=0.11效果巨差
```

下面是一个简单的神经网络，因为我确实没有怎么学过相关的调参方法，就只好手动调节损失函数和神经元个数。

```python
#构建模型
model = tf.keras.Sequential([
    tf.keras.layers.Dense(8,activation = 'relu', input_shape = (3,)),
    tf.keras.layers.Dense(64,activation = 'relu'),
    tf.keras.layers.Dense(16,activation = 'relu'),
    tf.keras.layers.Dense(1,activation = 'relu')
])
#相关训练指标，训练
lr = 0.001
opt = tf.keras.optimizers.SGD(learning_rate= lr)
model.compile(optimizer = opt, loss = 'mean_squared_error', metrics = ['accuracy'])
model.fit(X_train, y_train, epochs = 15, batch_size = 32, validation_split=0.2)
#评价模型
loss, accuracy = model.evaluate(X,y)
print("Test loss:",loss)
print("Test accuracy:", accuracy)
#Test loss: 0.31360727548599243
#Test accuracy: 0.5573770403862
```

## 用于预测的程序

```python
#用于获得前两天日期
def relative_time(use_now=True, date_string=None, date_format=None, years=0, months=0, days=0, leapdays=0, weeks=0, hours=0, minutes=0, seconds=0, microseconds=0):
    relative_delta = relativedelta(years=years, months=months, days=days, leapdays=leapdays, weeks=weeks, hours=hours,
                                   minutes=minutes, seconds=seconds, microseconds=microseconds)
    _date = datetime.datetime.now() if use_now else datetime.datetime.strptime(date_string, date_format)
    return (_date + relative_delta).strftime("%Y-%m-%d")
def relative_time_stock(use_now=True, date_string=None, date_format=None, years=0, months=0, days=0, leapdays=0, weeks=0, hours=0, minutes=0, seconds=0, microseconds=0):
    relative_delta = relativedelta(years=years, months=months, days=days, leapdays=leapdays, weeks=weeks, hours=hours,
                                   minutes=minutes, seconds=seconds, microseconds=microseconds)
    _date = datetime.datetime.now() if use_now else datetime.datetime.strptime(date_string, date_format)
    return (_date + relative_delta).strftime("%Y%m%d")
#读入评论数据，计算每条评论情绪值
dataset = pd.read_csv("002230.csv")
#计算每日情绪指标
def search_sense_daily(date):
    sum_search_pos = 0
    sum_search_neg = 0
    sum_tot = 0
    for i,row in dataset.iterrows():
        if(row['timetot'] == date):
            if(row['sense'] < 0.5):
                sum_search_pos = sum_search_pos + ( row['read'] + row['reply'] * 10)
            else:
                sum_search_neg = sum_search_neg + ( row['read'] + row['reply'] * 10)
    temp = (sum_search_pos + 1) / (sum_search_neg + 1)
    return math.log(temp, math.e)
#载入训练模型，预测结果
def fit(X_test):
    with open('KNN 2.pkl','rb') as f:
        KNN = pickle.load(f)
    with open('KNN tianchong 2.pkl','rb') as f:
        KNNtc = pickle.load(f)
    with open('Linear SVC 2.pkl','rb') as f:
        LinearSVC = pickle.load(f)
    with open('Linear SVC tianchong 2.pkl','rb') as f:
        LinearSVCtc = pickle.load(f)
    with open('LogReg 2.pkl','rb') as f:
        LogReg = pickle.load(f)
    with open('LogReg tianchong 2.pkl','rb') as f:
        LogRegtc = pickle.load(f)
    with open('Naive Bayes 2.pkl','rb') as f:
        Bayes = pickle.load(f)
    with open('Naive Bayes tianchong 2.pkl','rb') as f:
        Bayestc = pickle.load(f)
    with open('SVC 2.pkl','rb') as f:
        SVC = pickle.load(f)
    with open('SVC tianchong 2.pkl','rb') as f:
        SVCtc = pickle.load(f)
    tensor = tf.keras.models.load_model('tensorflow.h5')
    tensortc = tf.keras.models.load_model('tensorflowtc.h5')
    #X_test = np.array([per,sense1,sense2]).reshape(1,-1)
    print(KNNtc.predict(X_test),LinearSVCtc.predict(X_test),LogRegtc.predict(X_test),Bayestc.predict(X_test),SVCtc.predict(X_test),tensor.predict(X_test))
    print(KNN.predict(X_test),LinearSVC.predict(X_test),LogReg.predict(X_test),Bayes.predict(X_test),SVC.predict(X_test),tensortc.predict(X_test))
#以下为主函数
#改动input的值即可，要求想要预测的前一天有开盘
#第一行是前一天涨跌幅，前天情绪和昨天情绪的标准化后的结果
#解释：前五个代表不同模型的预测，最后一个为神经网络预测其为涨的概率
#0为跌，1为涨
input = "2024-05-16"
time1 = relative_time(use_now=False, date_string=input, date_format="%Y-%m-%d", days = -1)
time2 = relative_time(use_now=False, date_string=input, date_format="%Y-%m-%d", days = -2)
time1s = relative_time_stock(use_now=False, date_string=input, date_format="%Y-%m-%d", days = -1)
time2s = relative_time_stock(use_now=False, date_string=input, date_format="%Y-%m-%d", days = -2)
sense1 = search_sense_daily(time2)
sense2 = search_sense_daily(time1)
stock = ak.stock_zh_a_hist(symbol="002230", period="daily", start_date=time1s, end_date=time1s, adjust="")
per = stock.loc[0,'涨跌幅']
X_test = np.array([per,sense1,sense2]).reshape(1,-1)
#X_test = np.array([2,0.5,0.5]).reshape(1,-1)
std = load('std.joblib')
X_test = std.transform(X_test)
print(X_test)
fit(X_test)
```

